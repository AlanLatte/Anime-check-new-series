{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os,requests, re, webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(DB_filename: str, mode='r'):\n",
    "    \"\"\"\n",
    "        types:\n",
    "            data    —   tuple\n",
    "            index   —   int\n",
    "    \"\"\"\n",
    "    with open(DB_filename, mode) as raw_data:\n",
    "\n",
    "        data = raw_data.read().split('\\n')\n",
    "        index = data.index('')\n",
    "\n",
    "        if '' in data:  del data[index];\\\n",
    "                        return tuple(data)\n",
    "        else:           return tuple(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(urls: tuple, headers: dict):\n",
    "    \"\"\"\n",
    "        types:\n",
    "            url             —   str\n",
    "            index           —   int\n",
    "            headers         —   dict\n",
    "            error_message   —   str\n",
    "            result          —   str\n",
    "            title           —   str\n",
    "            series          —   str\n",
    "\n",
    "        information:\n",
    "            index for notify the user of any error\n",
    "    \"\"\"\n",
    "\n",
    "    result = str()\n",
    "    \"\"\"Start logic of parser \"\"\"\n",
    "\n",
    "    for index, url in enumerate(urls,1):\n",
    "        request = requests.get(url, headers=headers)\n",
    "        error_message = f'The {index} url was broken'\n",
    "        if request.status_code == 200:\n",
    "\n",
    "            soup = BeautifulSoup(request.content, 'html.parser')\n",
    "\n",
    "            if re.search(r'www.anilibria.tv', url):\n",
    "\n",
    "                title = soup.find(name='img', attrs={'id': 'adminPoster'}).get('alt')\n",
    "                try:\n",
    "                    series = re.search(r'(?<=((С|c)ерия 1-)).*(?= \\[)', soup.text).group()\n",
    "                except AttributeError:\n",
    "                    series = re.search(r'(\\d).*(?= \\[)', soup.text).group()\n",
    "                result+= f'{index} > {title} [{series}] <\\n'\n",
    "\n",
    "            elif re.search(r'anime.anidub.com', url):\n",
    "\n",
    "                title = soup.find(name='h1', attrs={'class': 'titlfull'}).renderContents().decode('utf-8')\n",
    "                result+= f'{index} > {title} <\\n'\n",
    "\n",
    "            else:\n",
    "                data = error_message\n",
    "                result+= f'{index} > {data} <\\n'\n",
    "        else:\n",
    "            return error_message\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def clear_data(file:str, mode='w', ):\n",
    "        with open(file, mode) as result:\n",
    "            result.write\n",
    "    def append_result(file:str, data:str, mode='a'):\n",
    "        with open(file, mode, encoding='utf-8') as result:\n",
    "            result.write(f'{data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data, DB_result, url):\n",
    "    \"\"\"\n",
    "        types:\n",
    "            old_data    —   tuple\n",
    "            new_data    —   tuple\n",
    "            result      —   list\n",
    "            index       —   int\n",
    "            check_mark  —   unicode string\n",
    "    \"\"\"\n",
    "    old_data = get_file_data(DB_filename = DB_result, mode='r')\n",
    "    Record.clear_data(file=DB_result, mode='w')\n",
    "    Record.append_result(file=DB_result, data=data, mode='a')\n",
    "    new_data = get_file_data(DB_filename = DB_result, mode='r')\n",
    "    if old_data != new_data:\n",
    "        result=list(set(new_data) - set(old_data))\n",
    "        for i in range(len(result)):\n",
    "            index = result[i][0]\n",
    "            check_mark = u'\\u2713'\n",
    "            print(f'{result[i]} available.[{check_mark}]')\n",
    "            webbrowser.open(str(url[int(index)-1]))\n",
    "    else:\n",
    "        print('New series didn`t come out :c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New series didn`t come out :c\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "        types:\n",
    "            DB_urls     —   str\n",
    "            DB_result   —   str\n",
    "            urls        —   tuple\n",
    "            headers     —   dict\n",
    "            data        —   str\n",
    "            LISTDIR     —   tuple\n",
    "\n",
    "        information:\n",
    "            DB          —   Data Base\n",
    "    \"\"\"\n",
    "\n",
    "    DB_urls     =   'urls.txt'\n",
    "    DB_result   =   'result.txt'\n",
    "    LISTDIR = tuple(os.listdir())\n",
    "    headers =   {\n",
    "                    'accept'     : '*/*',\n",
    "                    'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'\n",
    "                }\n",
    "    \"\"\" Check for files \"\"\"\n",
    "    # REVIEW: Доделать систему switch-case\n",
    "    if DB_result not in LISTDIR:\n",
    "        Record.clear_data(file=DB_result, mode='w')\n",
    "    if DB_urls not in LISTDIR:\n",
    "        Record.clear_data(file=DB_urls, mode='w')\n",
    "\n",
    "    urls = get_file_data(DB_filename = DB_urls, mode='r')\n",
    "\n",
    "    if urls:\n",
    "        data = parser(urls, headers)\n",
    "        check_data(data, DB_result, url=urls)\n",
    "    else:\n",
    "        print(f'Paste in {DB_urls} any urls')\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
